#数据预处理及每个文本的VSM表示
import os
import codecs
import chardet
import numpy as np


path='G:/PYCHARM/untitled3/20news-18828'
packs=os.listdir(path)
result1=[]
result2=[]
for pack in packs:
     path1=path+"/"+pack
     print(path1)
     files=os.listdir(path1)
     result1.append(path1)
     for file in files:
          path2=path1+"/"+file
          result2.append(path2)

print('文档数:',len(result2)/2) #如果该处不除以二，那么输出结果将为37656
----------------------------------------------------------------------------

#data pretreatment
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer as ss
stp = stopwords.words('english')
stm = ss('english')
symbols= [',','.',':','_','!','?','/','\'','\"','*','>','<','@','~','-','(',')','%','=','\\','^'
     ,'&','|','#','$','0','1','2','3','4','5','6','7','8','9','10','[',']','+','{','}',';','`','~']
for d_path in result2:
     if(d_path[-5:]!='_pret'):
         d = open(d_path,'rb')
         data = d.read()
         encode = chardet.detect(data)['encoding']
         with codecs.open(d_path, encoding=encode) as d:
             write_d = open(d_path+'_pret',"w+",encoding='utf-8') #ascii编码无法写入‘gbk’文件，把其改为utf-8编码
             words = d.read()
             for z in symbols:
                 words = words.replace(z,'')
             words = words.split()
             wordlist=[]
             for word in words:
                 word = stm.stem(word)
                 if word not in stp:
                     wordlist.append(word)

             write_d.writelines('\n'.join(wordlist))
             write_d.close()  #open后close是一个好习惯
----------------------------------------------------------------------------------------
glossary = {}
for d_path in result2:
     if(d_path[-5:]=='_pret'):
          d= open(d_path,'rb')
          words=d.read()
          words=words.split()
          for word in words:
               if glossary.get(word) == None:
                    glossary[word] = 1
               else:
                    glossary[word] += 1

print(glossary)

-------------------------------------------------------------------------------------------
#统计每个词语的与之相应的包含该词语之文件的数目
keys = list(glossary.keys())
word_vector = np.zeros(len(glossary))
for d_path in result2:
     if (d_path[-5:] == '_pret'):
          d=open(d_path,'rb')
          words=d.read()
          words = set(words.split())#集合中元素不具有重复性
          for word in words:
               word_vector[keys.index(word)] += 1
          d.close()
print(word_vector)             

#output：[9.5000e+01 7.0000e+01 1.8827e+04 ... 1.0000e+00 1.0000e+00 1.0000e+00]
-------------------------------------------------------------------------------------------
np.seterr(divide='ignore', invalid='ignore')
for item in glossary:
     idf_vec = np.log10(int(18828)/(word_vector+1))

#由于memory error问题，固选择一个文件夹一个文件夹的形式分步读取，相同形式的小段代码有20个，固只贴出其中一段
tfidf_vec_file='G:/PYCHARM/untitled3/vec_tf_idf.npy'
vec_multiple=[]
A=[]
path_1=path+"/"+"alt.atheism"
files=os.listdir(path_1)
for file in files:
    path__1=path_1+"/"+file
    A.append(path__1)
for d_path in A:
     if (d_path[-5:] == '_pret'):
          d=open(d_path,'rb')
          tf_vec = np.zeros((len(glossary)))
          words=d.read()
          words=words.split()
          keys=list(glossary.keys())
          for word in words:
               tf_vec[keys.index(word)]+=1
          tf_vec /= len(words)
          tf_idf_vec = tf_vec * idf_vec
          vec_multiple.append(tf_idf_vec)
          d.close()

np.save(tfidf_vec_file,vec_multiple)


